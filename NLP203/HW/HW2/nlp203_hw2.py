# -*- coding: utf-8 -*-
"""NLP203_HW2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16570S1JMoNPGuTbYwRWVOCl42qR4h3cf

# Part 1
"""

from transformers import pipeline

# 載入 RoBERTa fine-tuned on SQuAD 2.0
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# 測試用的長維基百科文章（DeepSeek 詳細內容）
deepseek_context = """
Hangzhou DeepSeek Artificial Intelligence Co., Ltd., trading as DeepSeek, is a Chinese artificial intelligence software company.
Its first product is an open-source large language model (LLM). It is based in Hangzhou, Zhejiang. It is owned and funded by
Chinese hedge fund High-Flyer. Its co-founder, Liang Wenfeng, established the company in 2023 and serves as its CEO.

The DeepSeek-R1 model provides responses comparable to other contemporary large language models, such as OpenAI's GPT-4o and
Meta’s LLaMA 3.1. The company claims that it trained DeepSeek-R1 for US$6 million compared to OpenAI's $100 million spent on
GPT-4 in 2023. DeepSeek's AI models were developed amid United States sanctions on China, restricting access to advanced chips
used to train LLMs.

DeepSeek's success against larger and more established rivals has been described as "upending AI". However, compliance with
Chinese government censorship policies and data collection practices has raised privacy concerns. In February 2024, Australia
banned the use of DeepSeek technology on all government devices.

DeepSeek was initially funded by High-Flyer, a hedge fund co-founded in 2016 by Liang Wenfeng, who had been using AI-powered
trading models since 2017. Before pivoting to AI, High-Flyer was a quantitative trading firm using deep learning techniques to
predict stock market movements. In 2019, Liang started acquiring Nvidia GPUs, anticipating the AI boom.

DeepSeek operates Fire-Flyer 2, a computing cluster with 5,000 Nvidia A100 GPUs. Initially, they exclusively used PCIe A100
GPUs, but later incorporated NVLinks to scale up training for larger models requiring model parallelism.

In January 2025, DeepSeek released its first chatbot app, DeepSeek-R1, on iOS and Android. By the end of the month, it had
surpassed ChatGPT as the most-downloaded app on the iOS App Store in the United States, triggering a sharp decline in Nvidia’s
stock price.

DeepSeek continues to develop AI models while maintaining an open-source approach. However, it enforces content restrictions in
accordance with local regulations, limiting responses on topics such as the Tiananmen Square massacre and Taiwan’s political
status. It has also recruited top AI researchers from Chinese universities to bolster its expertise.
"""

# 測試問題（讓模型可能出錯的問題）
questions = [
    "Who is the CEO of DeepSeek?",  # 應該回答 Liang Wenfeng
    "What is DeepSeek's first product?",  # 應該回答 open-source large language model (LLM)
    "Where is DeepSeek headquartered?",  # 應該回答 Hangzhou, Zhejiang
    "Which country banned DeepSeek's technology on government devices?",  # 應該回答 Australia
    "How many Nvidia GPUs does DeepSeek use?",  # 應該回答 5,000 A100 GPUs
    "What year was DeepSeek founded?",  # 應該回答 2023
    "How much did DeepSeek spend on training DeepSeek-R1?",  # 應該回答 $6 million
    "Who owns DeepSeek?",  # 應該回答 High-Flyer
    "What was High-Flyer's original business before AI?",  # 應該回答 quantitative trading
    "Which AI model did DeepSeek release in January 2025?",  # 應該回答 DeepSeek-R1
    "Which financial impact did DeepSeek cause in January 2025?",  # 應該回答 Nvidia's stock price dropped
    "What topics does DeepSeek restrict in its models?",  # 應該回答 Tiananmen Square and Taiwan's political status
    "What are DeepSeek's primary AI competitors?",  # 應該回答 OpenAI GPT-4o, Meta LLaMA 3.1
    "How much did OpenAI spend training GPT-4?",  # 應該回答 $100 million
    "What is the first smartphone made by DeepSeek?",  # 應該要回答 "No answer"（因為 DeepSeek 沒有做手機）
    "Which university did DeepSeek’s CEO graduate from?",  # 沒有資訊，應該回答 "No answer"
    "What is the name of DeepSeek's AI trading system?",  # 應該回答 "No answer"（因為 DeepSeek 沒有 AI 交易系統）
]

# 測試模型是否產生錯誤回答
print("\n=== In-Domain 維基百科文章測試 ===\n")
for question in questions:
    answer = qa_pipeline(question=question, context=deepseek_context)
    print(f"Q: {question}\nA: {answer['answer']}\n")
# 測試錯誤回答 (修改文章，使模型容易出錯)
modified_context = """
Hangzhou DeepSeek Artificial Intelligence Co., Ltd., trading as DeepSeek, is a smartphone manufacturer.
Its first product was the DeepPhone, which was released in 2023. The company was founded by Elon Musk
and is based in Shenzhen, China.
"""

print("\n=== 測試模型是否容易被誤導 ===\n")
misleading_questions = [
    "Who founded DeepSeek?",  # 預期應該回答 Elon Musk（錯誤）
    "What is DeepSeek's first product?",  # 預期應該回答 DeepPhone（錯誤）
    "Where is DeepSeek based?",  # 預期應該回答 Shenzhen, China（錯誤）
]

for question in misleading_questions:
    answer = qa_pipeline(question=question, context=modified_context)
    print(f"Q: {question}\nA: {answer['answer']}\n")

# 測試 Out-of-Domain (例如醫學文獻、詩歌)
out_of_domain_context = """
The coronavirus pandemic has affected millions worldwide. Researchers have found that mRNA vaccines
provide strong immunity. The first vaccine was developed in 2020 by Pfizer and BioNTech.
"""

print("\n=== Out-of-Domain 測試 (醫學文獻) ===\n")
ood_questions = [
    "Who founded DeepSeek?",  # 這篇文章完全無關 DeepSeek，模型應該回答 "No answer"
    "What is the first vaccine developed for COVID-19?",  # 應該能夠回答
  ]
for question in ood_questions:
    answer = qa_pipeline(question=question, context=out_of_domain_context)
    print(f"Q: {question}\nA: {answer['answer']}\n")

"""# Part 2"""

!pip install transformers datasets evaluate torch

import torch
import json
import evaluate
import numpy as np
from datasets import load_dataset
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer
# 載入 Covid-QA 訓練與開發集
dataset = load_dataset(
    "json",
    data_files={
        "train": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-train.json",
        "dev": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json"
    }
)

print("📌 訓練集第一筆數據：", dataset["train"][0])
# 使用 roberta-base-squad2 Tokenizer
model_checkpoint = "deepset/roberta-base-squad2"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)

def prepare_features(examples):
    """將問題 (question) 和文章內容 (context) 進行 Tokenization"""
    tokenized_examples = tokenizer(
        examples["question"], examples["context"],
        truncation="only_second",
        max_length=384,
        stride=128,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    offset_mapping = tokenized_examples.pop("offset_mapping")

    start_positions, end_positions = [], []

    for i, offsets in enumerate(offset_mapping):
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]

        if len(answers) == 0:
            start_positions.append(0)
            end_positions.append(0)
        else:
            answer = answers[0]
            answer_start = answer["answer_start"]
            answer_end = answer_start + len(answer["text"])

            sequence_ids = tokenized_examples.sequence_ids(i)
            token_start_index = next(idx for idx, sid in enumerate(sequence_ids) if sid == 1)
            token_end_index = len(offsets) - 1 - next(idx for idx, sid in enumerate(sequence_ids[::-1]) if sid == 1)

            if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):
                start_positions.append(0)
                end_positions.append(0)
            else:
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:
                    token_start_index += 1
                start_positions.append(token_start_index - 1)

                while token_end_index >= 0 and offsets[token_end_index][1] >= answer_end:
                    token_end_index -= 1
                end_positions.append(token_end_index + 1)

    tokenized_examples["start_positions"] = start_positions
    tokenized_examples["end_positions"] = end_positions
    return tokenized_examples

# Tokenize 訓練與開發集
tokenized_train = dataset["train"].map(prepare_features, batched=True, remove_columns=["question", "context", "id", "answers"])
tokenized_dev = dataset["dev"].map(prepare_features, batched=True, remove_columns=["question", "context", "id", "answers"])

print("📌 Tokenization 後的第一筆數據：", tokenized_train[0])
# 載入 RoBERTa QA 模型
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

# 訓練參數設定
training_args = TrainingArguments(
    output_dir="./qa_finetuned",
    evaluation_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,  # 設定為 0 就不會實際訓練
    weight_decay=0.01,
    save_total_limit=2,
    save_strategy="epoch",
    load_best_model_at_end=True,
    fp16=torch.cuda.is_available(),  # 如果有 GPU 就使用 fp16 訓練
    logging_dir="./logs",
)

# 定義評估函數
metric = evaluate.load("squad")

def compute_metrics(eval_preds):
    predictions, labels = eval_preds
    start_logits, end_logits = predictions
    start_positions, end_positions = labels

    predictions_dict = {
        "id": [],
        "prediction_text": [],
    }

    for idx in range(len(start_positions)):
        pred_start = np.argmax(start_logits[idx])
        pred_end = np.argmax(end_logits[idx])
        pred_text = tokenizer.decode(tokenized_dev[idx]["input_ids"][pred_start:pred_end + 1])
        predictions_dict["id"].append(str(idx))
        predictions_dict["prediction_text"].append(pred_text)

    return metric.compute(predictions=predictions_dict, references=dataset["dev"])

# 建立 Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_dev,
    compute_metrics=compute_metrics,
)

# 訓練模型
# trainer.train()


model.save_pretrained("/content/drive/MyDrive/HW2/qa_finetuned")
tokenizer.save_pretrained("/content/drive/MyDrive/HW2/qa_finetuned")

print("✅ 訓練完成，模型已儲存！")
def answer_question(model, tokenizer, question, context):
    """讓模型預測答案"""
    inputs = tokenizer(question, context, return_tensors="pt", truncation=True).to("cuda" if torch.cuda.is_available() else "cpu")

    with torch.no_grad():
        outputs = model(**inputs)

    start_logits, end_logits = outputs.start_logits, outputs.end_logits
    answer_start = torch.argmax(start_logits)
    answer_end = torch.argmax(end_logits) + 1

    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end]))

    return answer.strip()

# 產生 pred.json
predictions = {}

for entry in dataset["dev"]:
    for paragraph in entry["paragraphs"]:
        context = paragraph["context"]
        for qa in paragraph["qas"]:
            question = qa["question"]
            qid = qa["id"]
            predictions[qid] = answer_question(model, tokenizer, question, context)

# 儲存結果
with open("/content/drive/MyDrive/HW2/Part2_pred.json", "w", encoding="utf-8") as f:
    json.dump(predictions, f, indent=4)

print("✅ 預測結果已儲存至 Part2_pred.json")
python3 /content/drive/MyDrive/HW2/evaluate.py \
    /content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json \
    /content/drive/MyDrive/HW2/fixed_pred.json \
    --out-file /content/drive/MyDrive/HW2/eval.json

"""# Part **3**"""

# 掛載 Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 安裝必要的套件（如果尚未安裝）
!pip install transformers datasets adapter-transformers

import torch
import json
from datasets import load_dataset
from transformers import AutoTokenizer, TrainingArguments
from adapters import AutoAdapterModel, AdapterTrainer  # Adapter Transformers 套件

# 設定設備
device = "cuda" if torch.cuda.is_available() else "cpu"
print("使用設備：", device)

# -------------------------------
# 1. 載入 COVID-QA 資料集
# -------------------------------
dataset = load_dataset(
    "json",
    data_files={
        "train": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-train.json",
        "dev": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json"
    }
)

print("原始訓練資料第一筆：")
print(dataset["train"][0])

# -------------------------------
# 2. 扁平化 JSON 結構
# -------------------------------
def flatten_examples_batched(batch):
    """將 JSON 結構轉換為 `question`、`context`、`answers` 格式"""
    new_batch = {"question": [], "context": [], "id": [], "answers": []}
    for data_item in batch["data"]:
        for item in (data_item if isinstance(data_item, list) else [data_item]):
            for paragraph in item["paragraphs"]:
                context = paragraph["context"]
                for qa in paragraph["qas"]:
                    new_batch["question"].append(qa["question"])
                    new_batch["context"].append(context)
                    new_batch["id"].append(qa["id"])
                    new_batch["answers"].append(qa.get("answers", []))
    return new_batch

flat_train_dataset = dataset["train"].map(flatten_examples_batched, batched=True, remove_columns=["data"])
flat_dev_dataset = dataset["dev"].map(flatten_examples_batched, batched=True, remove_columns=["data"])

print("扁平化後訓練資料第一筆：")
print(flat_train_dataset[0])

# -------------------------------
# 3. Tokenization 與標註答案位置
# -------------------------------
tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2", use_fast=True)

def prepare_train_features(examples):
    """Tokenization 並標註答案起始與結束位置"""
    tokenized_examples = tokenizer(
        examples["question"],
        examples["context"],
        truncation="only_second",
        max_length=384,  # 確保不截斷答案
        stride=128,      # 避免過度切割
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    offset_mapping = tokenized_examples.pop("offset_mapping")

    start_positions, end_positions = [], []

    for i, offsets in enumerate(offset_mapping):
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]

        if len(answers) == 0:
            start_positions.append(0)
            end_positions.append(0)
        else:
            answer = answers[0]
            answer_start = answer["answer_start"]
            answer_end = answer_start + len(answer["text"])

            sequence_ids = tokenized_examples.sequence_ids(i)
            token_start_index = next(idx for idx, sid in enumerate(sequence_ids) if sid == 1)
            token_end_index = len(offsets) - 1 - next(idx for idx, sid in enumerate(sequence_ids[::-1]) if sid == 1)

            if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):
                start_positions.append(0)
                end_positions.append(0)
            else:
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:
                    token_start_index += 1
                start_positions.append(token_start_index - 1)

                while token_end_index >= 0 and offsets[token_end_index][1] >= answer_end:
                    token_end_index -= 1
                end_positions.append(token_end_index + 1)

    tokenized_examples["start_positions"] = start_positions
    tokenized_examples["end_positions"] = end_positions
    return tokenized_examples

tokenized_train = flat_train_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])
tokenized_dev = flat_dev_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])

print("Tokenization 後的訓練資料第一筆：")
print(tokenized_train[0])

# -------------------------------
# 4. Adapter 模型載入與訓練
# -------------------------------
adapter_model = AutoAdapterModel.from_pretrained("deepset/roberta-base-squad2")
adapter_model.add_adapter("qa_adapter", config="pfeiffer")
adapter_model.train_adapter("qa_adapter")
adapter_model.to(device)

training_args = TrainingArguments(
    output_dir="./adapter_results",
    evaluation_strategy="epoch",
    learning_rate=1e-4,  # 🔹 降低學習率，讓模型穩定學習
    per_device_train_batch_size=8,  # 🔹 降低 batch_size，避免過擬合
    per_device_eval_batch_size=8,
    num_train_epochs=3,  # 🔹 降低 epochs 數量，避免過擬合
    weight_decay=0.05,  # 🔹 添加正則化，防止參數過度收斂
    save_total_limit=2,
    save_strategy="epoch",
    load_best_model_at_end=True,
    fp16=True if device == "cuda" else False,
    warmup_steps=500,  # 🔹 增加 warmup，避免前期學習率過高
)


trainer = AdapterTrainer(
    model=adapter_model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_dev
)

trainer.train()

# 儲存訓練後的 Adapter 權重
adapter_model.save_adapter("/content/drive/MyDrive/adapter-qa", "qa_adapter")

# -------------------------------
# 5. 問答推論 (QA Inference)
# -------------------------------
adapter_model.load_adapter("/content/drive/MyDrive/adapter-qa", load_as="qa_adapter")

adapter_model.set_active_adapters(["qa_adapter"])  # 這裡要用 "qa_adapter"
adapter_model.to(device)

def answer_question(model, tokenizer, question, context):
    """改進的手動推論函數，確保正確處理 'No Answer' 狀況"""
    inputs = tokenizer(question, context, return_tensors="pt", truncation=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)

    start_logits = outputs.start_logits
    end_logits = outputs.end_logits

    answer_start = torch.argmax(start_logits)
    answer_end = torch.argmax(end_logits) + 1

    no_answer_score = start_logits[0][0] + end_logits[0][0]

    if no_answer_score > start_logits[0][answer_start] + end_logits[0][answer_end - 1]:
        return "No Answer"

    if answer_start >= len(inputs["input_ids"][0]) or answer_end > len(inputs["input_ids"][0]):
        return "No Answer"

    answer = tokenizer.convert_tokens_to_string(
        tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end])
    )

    return answer.strip()

# 預測結果
with open("/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json", "r", encoding="utf-8") as f:
    covid_qa_dev = json.load(f)

# 產生預測結果
predictions = {}

for entry in covid_qa_dev["data"]:
    for paragraph in entry["paragraphs"]:
        context = paragraph["context"]
        for qa in paragraph["qas"]:
            question = qa["question"]
            qid = qa["id"]
            # 取得模型答案
            predicted_answer = answer_question(adapter_model, tokenizer, question, context)
            # 存入字典，格式為 { "id": "answer" }
            predictions[qid] = predicted_answer
print(predictions)
# 儲存到 pred.json
pred_path = "/content/drive/MyDrive/HW2/fixed_pred.json"

with open(pred_path, "w", encoding="utf-8") as f:
    json.dump(predictions, f, indent=4)

print(f"✅ 預測結果已儲存至 {pred_path}")


!python3 /content/drive/MyDrive/HW2/evaluate.py \
    /content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json \
    /content/drive/MyDrive/HW2/fixed_pred.json \
    --out-file /content/drive/MyDrive/HW2/eval.json

with open("/content/drive/MyDrive/HW2/covid-qa/covid-qa-test.json", "r", encoding="utf-8") as f:
    covid_qa_dev = json.load(f)
predictions = {}

for entry in covid_qa_dev["data"]:
    for paragraph in entry["paragraphs"]:
        context = paragraph["context"]
        for qa in paragraph["qas"]:
            question = qa["question"]
            qid = qa["id"]
            predicted_answer = answer_question(adapter_model, tokenizer, question, context)
            predictions[qid] = predicted_answer
pred_path = "/content/drive/MyDrive/HW2/fixed_pred_test.json"

with open(pred_path, "w", encoding="utf-8") as f:
    json.dump(predictions, f, indent=4)

print(f"✅ 預測結果已儲存至 {pred_path}")
!python3 /content/drive/MyDrive/HW2/evaluate.py \
    /content/drive/MyDrive/HW2/covid-qa/covid-qa-test.json \
    /content/drive/MyDrive/HW2/fixed_pred_test.json \
    --out-file /content/drive/MyDrive/HW2/eval_test.json

"""# Test Code"""

# # 掛載 Google Drive
# from google.colab import drive
# drive.mount('/content/drive')

# # 安裝所需套件（如果尚未安裝）
# !pip install transformers datasets adapter-transformers

# import torch
# device = "cuda" if torch.cuda.is_available() else "cpu"
# print("使用設備：", device)

# import json
# from datasets import load_dataset
# from transformers import AutoTokenizer, TrainingArguments, pipeline
# from adapters import AutoAdapterModel, AdapterTrainer  # 請確認已安裝 adapter-transformers 套件

# # -------------------------------
# # 1. 載入原始資料
# # -------------------------------
# # 注意：根據你 Drive 上的實際路徑調整這裡的路徑
# dataset = load_dataset(
#     "json",
#     data_files={
#         "train": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-train.json",
#         "dev": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json"
#     }
# )

# print("原始訓練資料第一筆：")
# print(dataset["train"][0])

# # -------------------------------
# # 2. 扁平化資料 (使用 map 與 batched=True)
# # -------------------------------
# def flatten_examples_batched(batch):
#     """
#     將每一個批次中的原始資料扁平化，將每筆資料中的每組 question–context 配對提取出來。
#     假設原始資料結構如下：
#       {
#          "data": [
#              { "paragraphs": [ { "context": "...", "qas": [ { "question": "...", "id": "...", "answers": [...] }, ... ] }, ... ] },
#              ...
#          ]
#       }
#     """
#     new_batch = {"question": [], "context": [], "id": [], "answers": []}
#     for data_item in batch["data"]:
#         # 如果 data_item 是 list，則展開；否則包成 list
#         if isinstance(data_item, list):
#             items = data_item
#         else:
#             items = [data_item]
#         for item in items:
#             # item 應該是 dict，包含 key "paragraphs"
#             for paragraph in item["paragraphs"]:
#                 context = paragraph["context"]
#                 for qa in paragraph["qas"]:
#                     new_batch["question"].append(qa["question"])
#                     new_batch["context"].append(context)
#                     new_batch["id"].append(qa["id"])
#                     new_batch["answers"].append(qa.get("answers", []))
#     return new_batch

# flat_train_dataset = dataset["train"].map(flatten_examples_batched, batched=True, remove_columns=["data"])
# flat_dev_dataset = dataset["dev"].map(flatten_examples_batched, batched=True, remove_columns=["data"])

# print("扁平化後訓練資料第一筆：")
# print(flat_train_dataset[0])

# # -------------------------------
# # 3. Tokenization 與準備標籤
# # -------------------------------
# # 使用 fast tokenizer 加速 tokenization
# tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2", use_fast=True)

# def prepare_train_features(examples):
#     """
#     對每筆 QA 資料進行 tokenization，同時利用 offset_mapping 找出答案在 context 中的 token 起始與結束位置。
#     為了加速運算，將 max_length 降為 256，stride 降為 64。
#     """
#     tokenized_examples = tokenizer(
#         examples["question"],
#         examples["context"],
#         truncation="only_second",       # 只截斷 context 部分
#         max_length=512,                 # 調整 max_length
#         stride=128,                      # 調整 stride
#         return_overflowing_tokens=True,
#         return_offsets_mapping=True,
#         padding="max_length",
#     )

#     # 保存每筆 tokenized 結果對應原始樣本的索引
#     sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
#     offset_mapping = tokenized_examples.pop("offset_mapping")

#     start_positions = []
#     end_positions = []

#     for i, offsets in enumerate(offset_mapping):
#         sample_index = sample_mapping[i]
#         answers = examples["answers"][sample_index]  # answers 為 list（可能為空）
#         if len(answers) == 0:
#             start_positions.append(0)
#             end_positions.append(0)
#         else:
#             answer = answers[0]
#             answer_start = answer["answer_start"]
#             answer_text = answer["text"]
#             answer_end = answer_start + len(answer_text)

#             sequence_ids = tokenized_examples.sequence_ids(i)
#             token_start_index = 0
#             while token_start_index < len(offsets) and sequence_ids[token_start_index] != 1:
#                 token_start_index += 1
#             token_end_index = len(offsets) - 1
#             while token_end_index >= 0 and sequence_ids[token_end_index] != 1:
#                 token_end_index -= 1

#             if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):
#                 start_positions.append(0)
#                 end_positions.append(0)
#             else:
#                 while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:
#                     token_start_index += 1
#                 start_positions.append(token_start_index - 1)

#                 while token_end_index >= 0 and offsets[token_end_index][1] >= answer_end:
#                     token_end_index -= 1
#                 end_positions.append(token_end_index + 1)

#     tokenized_examples["start_positions"] = start_positions
#     tokenized_examples["end_positions"] = end_positions
#     return tokenized_examples

# tokenized_train = flat_train_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])
# tokenized_dev = flat_dev_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])

# print("Tokenization 與標籤準備後訓練資料第一筆：")
# print(tokenized_train[0])

# # -------------------------------
# # 4. Adapter 模型載入與訓練設定
# # -------------------------------
# adapter_model = AutoAdapterModel.from_pretrained("deepset/roberta-base-squad2")
# adapter_model.add_adapter("qa_adapter", config="pfeiffer")
# adapter_model.train_adapter("qa_adapter")  # 設定只訓練 adapter 部分
# adapter_model.to(device)

# training_args = TrainingArguments(
#     output_dir="./adapter_results",
#     evaluation_strategy="epoch",  # 如不需要頻繁評估，可改為 "no"
#     learning_rate=3e-3,
#     per_device_train_batch_size=16,
#     per_device_eval_batch_size=16,
#     num_train_epochs=10,           # 測試時可暫時降低 epoch 數
#     weight_decay=0.01,
#     save_total_limit=2,
#     save_strategy="epoch",
#     load_best_model_at_end=True,
#     fp16=True if device=="cuda" else False
# )

# trainer = AdapterTrainer(
#     model=adapter_model,
#     args=training_args,
#     train_dataset=tokenized_train,
#     eval_dataset=tokenized_dev
# )

# # 開始 Adapter 訓練
# trainer.train()

# # 儲存訓練後的 adapter 權重到 Drive
# adapter_model.save_adapter("/content/drive/MyDrive/adapter-qa", "qa_adapter")

# # -------------------------------
# # 5. 問答推論 (QA Inference)
# # -------------------------------
# adapter_model.load_adapter("/content/drive/MyDrive/adapter-qa", "qa_adapter")
# adapter_model.set_active_adapters("qa_adapter")
# qa_pipeline = pipeline("question-answering", model=adapter_model, tokenizer=tokenizer)

# with open("/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json", "r", encoding="utf-8") as f:
#     covid_qa_dev = json.load(f)

# predictions = {}
# for entry in covid_qa_dev["data"]:
#     for paragraph in entry["paragraphs"]:
#         context = paragraph["context"]
#         for qa in paragraph["qas"]:
#             question = qa["question"]
#             qid = qa["id"]
#             pred = qa_pipeline(question=question, context=context)
#             predictions[qid] = pred["answer"]

# with open("/content/drive/MyDrive/HW2/pred.json", "w", encoding="utf-8") as f:
#     json.dump(predictions, f, indent=4, ensure_ascii=False)

# print("前 5 筆推論結果：")
# for idx, (qid, answer) in enumerate(predictions.items()):
#     if idx >= 5:
#         break
#     print(f"ID: {qid} -> 答案: {answer}")

# # 掛載 Google Drive
# from google.colab import drive
# drive.mount('/content/drive')

# # 安裝所需套件（如果尚未安裝）
# !pip install transformers datasets adapter-transformers

# import torch
# import json
# from datasets import load_dataset
# from transformers import AutoTokenizer, TrainingArguments
# from adapters import AutoAdapterModel, AdapterTrainer  # Adapter Transformers 套件

# # 設定設備
# device = "cuda" if torch.cuda.is_available() else "cpu"
# print("使用設備：", device)

# # -------------------------------
# # 1. 載入 COVID-QA 資料集
# # -------------------------------
# dataset = load_dataset(
#     "json",
#     data_files={
#         "train": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-train.json",
#         "dev": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json"
#     }
# )

# print("原始訓練資料第一筆：")
# print(dataset["train"][0])

# # -------------------------------
# # 2. 扁平化 JSON 結構
# # -------------------------------
# def flatten_examples_batched(batch):
#     """將 JSON 結構轉換為 `question`、`context`、`answers` 格式"""
#     new_batch = {"question": [], "context": [], "id": [], "answers": []}
#     for data_item in batch["data"]:
#         for item in (data_item if isinstance(data_item, list) else [data_item]):
#             for paragraph in item["paragraphs"]:
#                 context = paragraph["context"]
#                 for qa in paragraph["qas"]:
#                     new_batch["question"].append(qa["question"])
#                     new_batch["context"].append(context)
#                     new_batch["id"].append(qa["id"])
#                     new_batch["answers"].append(qa.get("answers", []))
#     return new_batch

# flat_train_dataset = dataset["train"].map(flatten_examples_batched, batched=True, remove_columns=["data"])
# flat_dev_dataset = dataset["dev"].map(flatten_examples_batched, batched=True, remove_columns=["data"])

# print("扁平化後訓練資料第一筆：")
# print(flat_train_dataset[0])

# # -------------------------------
# # 3. Tokenization 與標註答案位置
# # -------------------------------
# tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2", use_fast=True)

# def prepare_train_features(examples):
#     """Tokenization，並標註答案起始與結束位置"""
#     tokenized_examples = tokenizer(
#         examples["question"],
#         examples["context"],
#         truncation="only_second",
#         max_length=512,  # 確保不截斷答案
#         stride=128,      # 避免過度切割
#         return_overflowing_tokens=True,
#         return_offsets_mapping=True,
#         padding="max_length",
#     )

#     sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
#     offset_mapping = tokenized_examples.pop("offset_mapping")

#     start_positions, end_positions = [], []

#     for i, offsets in enumerate(offset_mapping):
#         sample_index = sample_mapping[i]
#         answers = examples["answers"][sample_index]
#         if len(answers) == 0:
#             start_positions.append(0)
#             end_positions.append(0)
#         else:
#             answer = answers[0]
#             answer_start = answer["answer_start"]
#             answer_end = answer_start + len(answer["text"])

#             sequence_ids = tokenized_examples.sequence_ids(i)
#             token_start_index = next(idx for idx, sid in enumerate(sequence_ids) if sid == 1)
#             token_end_index = len(offsets) - 1 - next(idx for idx, sid in enumerate(sequence_ids[::-1]) if sid == 1)

#             if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):
#                 start_positions.append(0)
#                 end_positions.append(0)
#             else:
#                 while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:
#                     token_start_index += 1
#                 start_positions.append(token_start_index - 1)

#                 while token_end_index >= 0 and offsets[token_end_index][1] >= answer_end:
#                     token_end_index -= 1
#                 end_positions.append(token_end_index + 1)

#     tokenized_examples["start_positions"] = start_positions
#     tokenized_examples["end_positions"] = end_positions
#     return tokenized_examples

# tokenized_train = flat_train_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])
# tokenized_dev = flat_dev_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])

# print("Tokenization 後的訓練資料第一筆：")
# print(tokenized_train[0])

# # -------------------------------
# # 4. Adapter 模型載入與訓練
# # -------------------------------
# adapter_model = AutoAdapterModel.from_pretrained("deepset/roberta-base-squad2")
# adapter_model.add_adapter("qa_adapter", config="pfeiffer")
# adapter_model.train_adapter("qa_adapter")
# adapter_model.to(device)

# training_args = TrainingArguments(
#     output_dir="./adapter_results",
#     evaluation_strategy="epoch",
#     learning_rate=3e-3,
#     per_device_train_batch_size=16,
#     per_device_eval_batch_size=16,
#     num_train_epochs=10,
#     weight_decay=0.01,
#     save_total_limit=2,
#     save_strategy="epoch",
#     load_best_model_at_end=True,
#     fp16=True if device == "cuda" else False
# )

# trainer = AdapterTrainer(
#     model=adapter_model,
#     args=training_args,
#     train_dataset=tokenized_train,
#     eval_dataset=tokenized_dev
# )

# trainer.train()

# # 儲存訓練後的 Adapter 權重
# adapter_model.save_adapter("/content/drive/MyDrive/adapter-qa", "qa_adapter")

# # -------------------------------
# # 5. 問答推論 (QA Inference)
# # -------------------------------
# adapter_model.load_adapter("/content/drive/MyDrive/adapter-qa", "qa_adapter")
# adapter_model.set_active_adapters(["qa_adapter"])
# adapter_model.to(device)

# def answer_question(model, tokenizer, question, context):
#     """手動推論函數"""
#     inputs = tokenizer(question, context, return_tensors="pt", truncation=True).to(device)
#     with torch.no_grad():
#         outputs = model(**inputs)
#     answer_start = torch.argmax(outputs.start_logits)
#     answer_end = torch.argmax(outputs.end_logits) + 1
#     answer = tokenizer.convert_tokens_to_string(
#         tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end])
#     )
#     return answer

# with open("/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json", "r", encoding="utf-8") as f:
#     covid_qa_dev = json.load(f)

# predictions = {}
# for entry in covid_qa_dev["data"]:
#     for paragraph in entry["paragraphs"]:
#         context = paragraph["context"]
#         for qa in paragraph["qas"]:
#             question = qa["question"]
#             qid = qa["id"]
#             pred_answer = answer_question(adapter_model, tokenizer, question, context)
#             predictions[qid] = pred_answer

# # 儲存預測結果
# with open("/content/drive/MyDrive/HW2/pred.json", "w", encoding="utf-8") as f:
#     json.dump(predictions, f, indent=4, ensure_ascii=False)

# # 顯示前 5 筆結果
# print("前 5 筆推論結果：")
# for idx, (qid, answer) in enumerate(predictions.items()):
#     if idx >= 5:
#         break
#     print(f"ID: {qid} -> 答案: {answer}")