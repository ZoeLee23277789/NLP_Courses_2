# -*- coding: utf-8 -*-
"""NLP203_HW2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16570S1JMoNPGuTbYwRWVOCl42qR4h3cf

# Part 1
"""

from transformers import pipeline

# è¼‰å…¥ RoBERTa fine-tuned on SQuAD 2.0
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# æ¸¬è©¦ç”¨çš„é•·ç¶­åŸºç™¾ç§‘æ–‡ç« ï¼ˆDeepSeek è©³ç´°å…§å®¹ï¼‰
deepseek_context = """
Hangzhou DeepSeek Artificial Intelligence Co., Ltd., trading as DeepSeek, is a Chinese artificial intelligence software company.
Its first product is an open-source large language model (LLM). It is based in Hangzhou, Zhejiang. It is owned and funded by
Chinese hedge fund High-Flyer. Its co-founder, Liang Wenfeng, established the company in 2023 and serves as its CEO.

The DeepSeek-R1 model provides responses comparable to other contemporary large language models, such as OpenAI's GPT-4o and
Metaâ€™s LLaMA 3.1. The company claims that it trained DeepSeek-R1 for US$6 million compared to OpenAI's $100 million spent on
GPT-4 in 2023. DeepSeek's AI models were developed amid United States sanctions on China, restricting access to advanced chips
used to train LLMs.

DeepSeek's success against larger and more established rivals has been described as "upending AI". However, compliance with
Chinese government censorship policies and data collection practices has raised privacy concerns. In February 2024, Australia
banned the use of DeepSeek technology on all government devices.

DeepSeek was initially funded by High-Flyer, a hedge fund co-founded in 2016 by Liang Wenfeng, who had been using AI-powered
trading models since 2017. Before pivoting to AI, High-Flyer was a quantitative trading firm using deep learning techniques to
predict stock market movements. In 2019, Liang started acquiring Nvidia GPUs, anticipating the AI boom.

DeepSeek operates Fire-Flyer 2, a computing cluster with 5,000 Nvidia A100 GPUs. Initially, they exclusively used PCIe A100
GPUs, but later incorporated NVLinks to scale up training for larger models requiring model parallelism.

In January 2025, DeepSeek released its first chatbot app, DeepSeek-R1, on iOS and Android. By the end of the month, it had
surpassed ChatGPT as the most-downloaded app on the iOS App Store in the United States, triggering a sharp decline in Nvidiaâ€™s
stock price.

DeepSeek continues to develop AI models while maintaining an open-source approach. However, it enforces content restrictions in
accordance with local regulations, limiting responses on topics such as the Tiananmen Square massacre and Taiwanâ€™s political
status. It has also recruited top AI researchers from Chinese universities to bolster its expertise.
"""

# æ¸¬è©¦å•é¡Œï¼ˆè®“æ¨¡å‹å¯èƒ½å‡ºéŒ¯çš„å•é¡Œï¼‰
questions = [
    "Who is the CEO of DeepSeek?",  # æ‡‰è©²å›ç­” Liang Wenfeng
    "What is DeepSeek's first product?",  # æ‡‰è©²å›ç­” open-source large language model (LLM)
    "Where is DeepSeek headquartered?",  # æ‡‰è©²å›ç­” Hangzhou, Zhejiang
    "Which country banned DeepSeek's technology on government devices?",  # æ‡‰è©²å›ç­” Australia
    "How many Nvidia GPUs does DeepSeek use?",  # æ‡‰è©²å›ç­” 5,000 A100 GPUs
    "What year was DeepSeek founded?",  # æ‡‰è©²å›ç­” 2023
    "How much did DeepSeek spend on training DeepSeek-R1?",  # æ‡‰è©²å›ç­” $6 million
    "Who owns DeepSeek?",  # æ‡‰è©²å›ç­” High-Flyer
    "What was High-Flyer's original business before AI?",  # æ‡‰è©²å›ç­” quantitative trading
    "Which AI model did DeepSeek release in January 2025?",  # æ‡‰è©²å›ç­” DeepSeek-R1
    "Which financial impact did DeepSeek cause in January 2025?",  # æ‡‰è©²å›ç­” Nvidia's stock price dropped
    "What topics does DeepSeek restrict in its models?",  # æ‡‰è©²å›ç­” Tiananmen Square and Taiwan's political status
    "What are DeepSeek's primary AI competitors?",  # æ‡‰è©²å›ç­” OpenAI GPT-4o, Meta LLaMA 3.1
    "How much did OpenAI spend training GPT-4?",  # æ‡‰è©²å›ç­” $100 million
    "What is the first smartphone made by DeepSeek?",  # æ‡‰è©²è¦å›ç­” "No answer"ï¼ˆå› ç‚º DeepSeek æ²’æœ‰åšæ‰‹æ©Ÿï¼‰
    "Which university did DeepSeekâ€™s CEO graduate from?",  # æ²’æœ‰è³‡è¨Šï¼Œæ‡‰è©²å›ç­” "No answer"
    "What is the name of DeepSeek's AI trading system?",  # æ‡‰è©²å›ç­” "No answer"ï¼ˆå› ç‚º DeepSeek æ²’æœ‰ AI äº¤æ˜“ç³»çµ±ï¼‰
]

# æ¸¬è©¦æ¨¡å‹æ˜¯å¦ç”¢ç”ŸéŒ¯èª¤å›ç­”
print("\n=== In-Domain ç¶­åŸºç™¾ç§‘æ–‡ç« æ¸¬è©¦ ===\n")
for question in questions:
    answer = qa_pipeline(question=question, context=deepseek_context)
    print(f"Q: {question}\nA: {answer['answer']}\n")
# æ¸¬è©¦éŒ¯èª¤å›ç­” (ä¿®æ”¹æ–‡ç« ï¼Œä½¿æ¨¡å‹å®¹æ˜“å‡ºéŒ¯)
modified_context = """
Hangzhou DeepSeek Artificial Intelligence Co., Ltd., trading as DeepSeek, is a smartphone manufacturer.
Its first product was the DeepPhone, which was released in 2023. The company was founded by Elon Musk
and is based in Shenzhen, China.
"""

print("\n=== æ¸¬è©¦æ¨¡å‹æ˜¯å¦å®¹æ˜“è¢«èª¤å° ===\n")
misleading_questions = [
    "Who founded DeepSeek?",  # é æœŸæ‡‰è©²å›ç­” Elon Muskï¼ˆéŒ¯èª¤ï¼‰
    "What is DeepSeek's first product?",  # é æœŸæ‡‰è©²å›ç­” DeepPhoneï¼ˆéŒ¯èª¤ï¼‰
    "Where is DeepSeek based?",  # é æœŸæ‡‰è©²å›ç­” Shenzhen, Chinaï¼ˆéŒ¯èª¤ï¼‰
]

for question in misleading_questions:
    answer = qa_pipeline(question=question, context=modified_context)
    print(f"Q: {question}\nA: {answer['answer']}\n")

# æ¸¬è©¦ Out-of-Domain (ä¾‹å¦‚é†«å­¸æ–‡ç»ã€è©©æ­Œ)
out_of_domain_context = """
The coronavirus pandemic has affected millions worldwide. Researchers have found that mRNA vaccines
provide strong immunity. The first vaccine was developed in 2020 by Pfizer and BioNTech.
"""

print("\n=== Out-of-Domain æ¸¬è©¦ (é†«å­¸æ–‡ç») ===\n")
ood_questions = [
    "Who founded DeepSeek?",  # é€™ç¯‡æ–‡ç« å®Œå…¨ç„¡é—œ DeepSeekï¼Œæ¨¡å‹æ‡‰è©²å›ç­” "No answer"
    "What is the first vaccine developed for COVID-19?",  # æ‡‰è©²èƒ½å¤ å›ç­”
  ]
for question in ood_questions:
    answer = qa_pipeline(question=question, context=out_of_domain_context)
    print(f"Q: {question}\nA: {answer['answer']}\n")

"""# Part 2"""

!pip install transformers datasets evaluate torch

import torch
import json
import evaluate
import numpy as np
from datasets import load_dataset
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer
# è¼‰å…¥ Covid-QA è¨“ç·´èˆ‡é–‹ç™¼é›†
dataset = load_dataset(
    "json",
    data_files={
        "train": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-train.json",
        "dev": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json"
    }
)

print("ğŸ“Œ è¨“ç·´é›†ç¬¬ä¸€ç­†æ•¸æ“šï¼š", dataset["train"][0])
# ä½¿ç”¨ roberta-base-squad2 Tokenizer
model_checkpoint = "deepset/roberta-base-squad2"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)

def prepare_features(examples):
    """å°‡å•é¡Œ (question) å’Œæ–‡ç« å…§å®¹ (context) é€²è¡Œ Tokenization"""
    tokenized_examples = tokenizer(
        examples["question"], examples["context"],
        truncation="only_second",
        max_length=384,
        stride=128,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    offset_mapping = tokenized_examples.pop("offset_mapping")

    start_positions, end_positions = [], []

    for i, offsets in enumerate(offset_mapping):
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]

        if len(answers) == 0:
            start_positions.append(0)
            end_positions.append(0)
        else:
            answer = answers[0]
            answer_start = answer["answer_start"]
            answer_end = answer_start + len(answer["text"])

            sequence_ids = tokenized_examples.sequence_ids(i)
            token_start_index = next(idx for idx, sid in enumerate(sequence_ids) if sid == 1)
            token_end_index = len(offsets) - 1 - next(idx for idx, sid in enumerate(sequence_ids[::-1]) if sid == 1)

            if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):
                start_positions.append(0)
                end_positions.append(0)
            else:
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:
                    token_start_index += 1
                start_positions.append(token_start_index - 1)

                while token_end_index >= 0 and offsets[token_end_index][1] >= answer_end:
                    token_end_index -= 1
                end_positions.append(token_end_index + 1)

    tokenized_examples["start_positions"] = start_positions
    tokenized_examples["end_positions"] = end_positions
    return tokenized_examples

# Tokenize è¨“ç·´èˆ‡é–‹ç™¼é›†
tokenized_train = dataset["train"].map(prepare_features, batched=True, remove_columns=["question", "context", "id", "answers"])
tokenized_dev = dataset["dev"].map(prepare_features, batched=True, remove_columns=["question", "context", "id", "answers"])

print("ğŸ“Œ Tokenization å¾Œçš„ç¬¬ä¸€ç­†æ•¸æ“šï¼š", tokenized_train[0])
# è¼‰å…¥ RoBERTa QA æ¨¡å‹
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

# è¨“ç·´åƒæ•¸è¨­å®š
training_args = TrainingArguments(
    output_dir="./qa_finetuned",
    evaluation_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,  # è¨­å®šç‚º 0 å°±ä¸æœƒå¯¦éš›è¨“ç·´
    weight_decay=0.01,
    save_total_limit=2,
    save_strategy="epoch",
    load_best_model_at_end=True,
    fp16=torch.cuda.is_available(),  # å¦‚æœæœ‰ GPU å°±ä½¿ç”¨ fp16 è¨“ç·´
    logging_dir="./logs",
)

# å®šç¾©è©•ä¼°å‡½æ•¸
metric = evaluate.load("squad")

def compute_metrics(eval_preds):
    predictions, labels = eval_preds
    start_logits, end_logits = predictions
    start_positions, end_positions = labels

    predictions_dict = {
        "id": [],
        "prediction_text": [],
    }

    for idx in range(len(start_positions)):
        pred_start = np.argmax(start_logits[idx])
        pred_end = np.argmax(end_logits[idx])
        pred_text = tokenizer.decode(tokenized_dev[idx]["input_ids"][pred_start:pred_end + 1])
        predictions_dict["id"].append(str(idx))
        predictions_dict["prediction_text"].append(pred_text)

    return metric.compute(predictions=predictions_dict, references=dataset["dev"])

# å»ºç«‹ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_dev,
    compute_metrics=compute_metrics,
)

# è¨“ç·´æ¨¡å‹
# trainer.train()


model.save_pretrained("/content/drive/MyDrive/HW2/qa_finetuned")
tokenizer.save_pretrained("/content/drive/MyDrive/HW2/qa_finetuned")

print("âœ… è¨“ç·´å®Œæˆï¼Œæ¨¡å‹å·²å„²å­˜ï¼")
def answer_question(model, tokenizer, question, context):
    """è®“æ¨¡å‹é æ¸¬ç­”æ¡ˆ"""
    inputs = tokenizer(question, context, return_tensors="pt", truncation=True).to("cuda" if torch.cuda.is_available() else "cpu")

    with torch.no_grad():
        outputs = model(**inputs)

    start_logits, end_logits = outputs.start_logits, outputs.end_logits
    answer_start = torch.argmax(start_logits)
    answer_end = torch.argmax(end_logits) + 1

    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end]))

    return answer.strip()

# ç”¢ç”Ÿ pred.json
predictions = {}

for entry in dataset["dev"]:
    for paragraph in entry["paragraphs"]:
        context = paragraph["context"]
        for qa in paragraph["qas"]:
            question = qa["question"]
            qid = qa["id"]
            predictions[qid] = answer_question(model, tokenizer, question, context)

# å„²å­˜çµæœ
with open("/content/drive/MyDrive/HW2/Part2_pred.json", "w", encoding="utf-8") as f:
    json.dump(predictions, f, indent=4)

print("âœ… é æ¸¬çµæœå·²å„²å­˜è‡³ Part2_pred.json")
python3 /content/drive/MyDrive/HW2/evaluate.py \
    /content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json \
    /content/drive/MyDrive/HW2/fixed_pred.json \
    --out-file /content/drive/MyDrive/HW2/eval.json

"""# Part **3**"""

# æ›è¼‰ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# å®‰è£å¿…è¦çš„å¥—ä»¶ï¼ˆå¦‚æœå°šæœªå®‰è£ï¼‰
!pip install transformers datasets adapter-transformers

import torch
import json
from datasets import load_dataset
from transformers import AutoTokenizer, TrainingArguments
from adapters import AutoAdapterModel, AdapterTrainer  # Adapter Transformers å¥—ä»¶

# è¨­å®šè¨­å‚™
device = "cuda" if torch.cuda.is_available() else "cpu"
print("ä½¿ç”¨è¨­å‚™ï¼š", device)

# -------------------------------
# 1. è¼‰å…¥ COVID-QA è³‡æ–™é›†
# -------------------------------
dataset = load_dataset(
    "json",
    data_files={
        "train": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-train.json",
        "dev": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json"
    }
)

print("åŸå§‹è¨“ç·´è³‡æ–™ç¬¬ä¸€ç­†ï¼š")
print(dataset["train"][0])

# -------------------------------
# 2. æ‰å¹³åŒ– JSON çµæ§‹
# -------------------------------
def flatten_examples_batched(batch):
    """å°‡ JSON çµæ§‹è½‰æ›ç‚º `question`ã€`context`ã€`answers` æ ¼å¼"""
    new_batch = {"question": [], "context": [], "id": [], "answers": []}
    for data_item in batch["data"]:
        for item in (data_item if isinstance(data_item, list) else [data_item]):
            for paragraph in item["paragraphs"]:
                context = paragraph["context"]
                for qa in paragraph["qas"]:
                    new_batch["question"].append(qa["question"])
                    new_batch["context"].append(context)
                    new_batch["id"].append(qa["id"])
                    new_batch["answers"].append(qa.get("answers", []))
    return new_batch

flat_train_dataset = dataset["train"].map(flatten_examples_batched, batched=True, remove_columns=["data"])
flat_dev_dataset = dataset["dev"].map(flatten_examples_batched, batched=True, remove_columns=["data"])

print("æ‰å¹³åŒ–å¾Œè¨“ç·´è³‡æ–™ç¬¬ä¸€ç­†ï¼š")
print(flat_train_dataset[0])

# -------------------------------
# 3. Tokenization èˆ‡æ¨™è¨»ç­”æ¡ˆä½ç½®
# -------------------------------
tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2", use_fast=True)

def prepare_train_features(examples):
    """Tokenization ä¸¦æ¨™è¨»ç­”æ¡ˆèµ·å§‹èˆ‡çµæŸä½ç½®"""
    tokenized_examples = tokenizer(
        examples["question"],
        examples["context"],
        truncation="only_second",
        max_length=384,  # ç¢ºä¿ä¸æˆªæ–·ç­”æ¡ˆ
        stride=128,      # é¿å…éåº¦åˆ‡å‰²
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    offset_mapping = tokenized_examples.pop("offset_mapping")

    start_positions, end_positions = [], []

    for i, offsets in enumerate(offset_mapping):
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]

        if len(answers) == 0:
            start_positions.append(0)
            end_positions.append(0)
        else:
            answer = answers[0]
            answer_start = answer["answer_start"]
            answer_end = answer_start + len(answer["text"])

            sequence_ids = tokenized_examples.sequence_ids(i)
            token_start_index = next(idx for idx, sid in enumerate(sequence_ids) if sid == 1)
            token_end_index = len(offsets) - 1 - next(idx for idx, sid in enumerate(sequence_ids[::-1]) if sid == 1)

            if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):
                start_positions.append(0)
                end_positions.append(0)
            else:
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:
                    token_start_index += 1
                start_positions.append(token_start_index - 1)

                while token_end_index >= 0 and offsets[token_end_index][1] >= answer_end:
                    token_end_index -= 1
                end_positions.append(token_end_index + 1)

    tokenized_examples["start_positions"] = start_positions
    tokenized_examples["end_positions"] = end_positions
    return tokenized_examples

tokenized_train = flat_train_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])
tokenized_dev = flat_dev_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])

print("Tokenization å¾Œçš„è¨“ç·´è³‡æ–™ç¬¬ä¸€ç­†ï¼š")
print(tokenized_train[0])

# -------------------------------
# 4. Adapter æ¨¡å‹è¼‰å…¥èˆ‡è¨“ç·´
# -------------------------------
adapter_model = AutoAdapterModel.from_pretrained("deepset/roberta-base-squad2")
adapter_model.add_adapter("qa_adapter", config="pfeiffer")
adapter_model.train_adapter("qa_adapter")
adapter_model.to(device)

training_args = TrainingArguments(
    output_dir="./adapter_results",
    evaluation_strategy="epoch",
    learning_rate=1e-4,  # ğŸ”¹ é™ä½å­¸ç¿’ç‡ï¼Œè®“æ¨¡å‹ç©©å®šå­¸ç¿’
    per_device_train_batch_size=8,  # ğŸ”¹ é™ä½ batch_sizeï¼Œé¿å…éæ“¬åˆ
    per_device_eval_batch_size=8,
    num_train_epochs=3,  # ğŸ”¹ é™ä½ epochs æ•¸é‡ï¼Œé¿å…éæ“¬åˆ
    weight_decay=0.05,  # ğŸ”¹ æ·»åŠ æ­£å‰‡åŒ–ï¼Œé˜²æ­¢åƒæ•¸éåº¦æ”¶æ–‚
    save_total_limit=2,
    save_strategy="epoch",
    load_best_model_at_end=True,
    fp16=True if device == "cuda" else False,
    warmup_steps=500,  # ğŸ”¹ å¢åŠ  warmupï¼Œé¿å…å‰æœŸå­¸ç¿’ç‡éé«˜
)


trainer = AdapterTrainer(
    model=adapter_model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_dev
)

trainer.train()

# å„²å­˜è¨“ç·´å¾Œçš„ Adapter æ¬Šé‡
adapter_model.save_adapter("/content/drive/MyDrive/adapter-qa", "qa_adapter")

# -------------------------------
# 5. å•ç­”æ¨è«– (QA Inference)
# -------------------------------
adapter_model.load_adapter("/content/drive/MyDrive/adapter-qa", load_as="qa_adapter")

adapter_model.set_active_adapters(["qa_adapter"])  # é€™è£¡è¦ç”¨ "qa_adapter"
adapter_model.to(device)

def answer_question(model, tokenizer, question, context):
    """æ”¹é€²çš„æ‰‹å‹•æ¨è«–å‡½æ•¸ï¼Œç¢ºä¿æ­£ç¢ºè™•ç† 'No Answer' ç‹€æ³"""
    inputs = tokenizer(question, context, return_tensors="pt", truncation=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)

    start_logits = outputs.start_logits
    end_logits = outputs.end_logits

    answer_start = torch.argmax(start_logits)
    answer_end = torch.argmax(end_logits) + 1

    no_answer_score = start_logits[0][0] + end_logits[0][0]

    if no_answer_score > start_logits[0][answer_start] + end_logits[0][answer_end - 1]:
        return "No Answer"

    if answer_start >= len(inputs["input_ids"][0]) or answer_end > len(inputs["input_ids"][0]):
        return "No Answer"

    answer = tokenizer.convert_tokens_to_string(
        tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end])
    )

    return answer.strip()

# é æ¸¬çµæœ
with open("/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json", "r", encoding="utf-8") as f:
    covid_qa_dev = json.load(f)

# ç”¢ç”Ÿé æ¸¬çµæœ
predictions = {}

for entry in covid_qa_dev["data"]:
    for paragraph in entry["paragraphs"]:
        context = paragraph["context"]
        for qa in paragraph["qas"]:
            question = qa["question"]
            qid = qa["id"]
            # å–å¾—æ¨¡å‹ç­”æ¡ˆ
            predicted_answer = answer_question(adapter_model, tokenizer, question, context)
            # å­˜å…¥å­—å…¸ï¼Œæ ¼å¼ç‚º { "id": "answer" }
            predictions[qid] = predicted_answer
print(predictions)
# å„²å­˜åˆ° pred.json
pred_path = "/content/drive/MyDrive/HW2/fixed_pred.json"

with open(pred_path, "w", encoding="utf-8") as f:
    json.dump(predictions, f, indent=4)

print(f"âœ… é æ¸¬çµæœå·²å„²å­˜è‡³ {pred_path}")


!python3 /content/drive/MyDrive/HW2/evaluate.py \
    /content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json \
    /content/drive/MyDrive/HW2/fixed_pred.json \
    --out-file /content/drive/MyDrive/HW2/eval.json

with open("/content/drive/MyDrive/HW2/covid-qa/covid-qa-test.json", "r", encoding="utf-8") as f:
    covid_qa_dev = json.load(f)
predictions = {}

for entry in covid_qa_dev["data"]:
    for paragraph in entry["paragraphs"]:
        context = paragraph["context"]
        for qa in paragraph["qas"]:
            question = qa["question"]
            qid = qa["id"]
            predicted_answer = answer_question(adapter_model, tokenizer, question, context)
            predictions[qid] = predicted_answer
pred_path = "/content/drive/MyDrive/HW2/fixed_pred_test.json"

with open(pred_path, "w", encoding="utf-8") as f:
    json.dump(predictions, f, indent=4)

print(f"âœ… é æ¸¬çµæœå·²å„²å­˜è‡³ {pred_path}")
!python3 /content/drive/MyDrive/HW2/evaluate.py \
    /content/drive/MyDrive/HW2/covid-qa/covid-qa-test.json \
    /content/drive/MyDrive/HW2/fixed_pred_test.json \
    --out-file /content/drive/MyDrive/HW2/eval_test.json

"""# Test Code"""

# # æ›è¼‰ Google Drive
# from google.colab import drive
# drive.mount('/content/drive')

# # å®‰è£æ‰€éœ€å¥—ä»¶ï¼ˆå¦‚æœå°šæœªå®‰è£ï¼‰
# !pip install transformers datasets adapter-transformers

# import torch
# device = "cuda" if torch.cuda.is_available() else "cpu"
# print("ä½¿ç”¨è¨­å‚™ï¼š", device)

# import json
# from datasets import load_dataset
# from transformers import AutoTokenizer, TrainingArguments, pipeline
# from adapters import AutoAdapterModel, AdapterTrainer  # è«‹ç¢ºèªå·²å®‰è£ adapter-transformers å¥—ä»¶

# # -------------------------------
# # 1. è¼‰å…¥åŸå§‹è³‡æ–™
# # -------------------------------
# # æ³¨æ„ï¼šæ ¹æ“šä½  Drive ä¸Šçš„å¯¦éš›è·¯å¾‘èª¿æ•´é€™è£¡çš„è·¯å¾‘
# dataset = load_dataset(
#     "json",
#     data_files={
#         "train": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-train.json",
#         "dev": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json"
#     }
# )

# print("åŸå§‹è¨“ç·´è³‡æ–™ç¬¬ä¸€ç­†ï¼š")
# print(dataset["train"][0])

# # -------------------------------
# # 2. æ‰å¹³åŒ–è³‡æ–™ (ä½¿ç”¨ map èˆ‡ batched=True)
# # -------------------------------
# def flatten_examples_batched(batch):
#     """
#     å°‡æ¯ä¸€å€‹æ‰¹æ¬¡ä¸­çš„åŸå§‹è³‡æ–™æ‰å¹³åŒ–ï¼Œå°‡æ¯ç­†è³‡æ–™ä¸­çš„æ¯çµ„ questionâ€“context é…å°æå–å‡ºä¾†ã€‚
#     å‡è¨­åŸå§‹è³‡æ–™çµæ§‹å¦‚ä¸‹ï¼š
#       {
#          "data": [
#              { "paragraphs": [ { "context": "...", "qas": [ { "question": "...", "id": "...", "answers": [...] }, ... ] }, ... ] },
#              ...
#          ]
#       }
#     """
#     new_batch = {"question": [], "context": [], "id": [], "answers": []}
#     for data_item in batch["data"]:
#         # å¦‚æœ data_item æ˜¯ listï¼Œå‰‡å±•é–‹ï¼›å¦å‰‡åŒ…æˆ list
#         if isinstance(data_item, list):
#             items = data_item
#         else:
#             items = [data_item]
#         for item in items:
#             # item æ‡‰è©²æ˜¯ dictï¼ŒåŒ…å« key "paragraphs"
#             for paragraph in item["paragraphs"]:
#                 context = paragraph["context"]
#                 for qa in paragraph["qas"]:
#                     new_batch["question"].append(qa["question"])
#                     new_batch["context"].append(context)
#                     new_batch["id"].append(qa["id"])
#                     new_batch["answers"].append(qa.get("answers", []))
#     return new_batch

# flat_train_dataset = dataset["train"].map(flatten_examples_batched, batched=True, remove_columns=["data"])
# flat_dev_dataset = dataset["dev"].map(flatten_examples_batched, batched=True, remove_columns=["data"])

# print("æ‰å¹³åŒ–å¾Œè¨“ç·´è³‡æ–™ç¬¬ä¸€ç­†ï¼š")
# print(flat_train_dataset[0])

# # -------------------------------
# # 3. Tokenization èˆ‡æº–å‚™æ¨™ç±¤
# # -------------------------------
# # ä½¿ç”¨ fast tokenizer åŠ é€Ÿ tokenization
# tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2", use_fast=True)

# def prepare_train_features(examples):
#     """
#     å°æ¯ç­† QA è³‡æ–™é€²è¡Œ tokenizationï¼ŒåŒæ™‚åˆ©ç”¨ offset_mapping æ‰¾å‡ºç­”æ¡ˆåœ¨ context ä¸­çš„ token èµ·å§‹èˆ‡çµæŸä½ç½®ã€‚
#     ç‚ºäº†åŠ é€Ÿé‹ç®—ï¼Œå°‡ max_length é™ç‚º 256ï¼Œstride é™ç‚º 64ã€‚
#     """
#     tokenized_examples = tokenizer(
#         examples["question"],
#         examples["context"],
#         truncation="only_second",       # åªæˆªæ–· context éƒ¨åˆ†
#         max_length=512,                 # èª¿æ•´ max_length
#         stride=128,                      # èª¿æ•´ stride
#         return_overflowing_tokens=True,
#         return_offsets_mapping=True,
#         padding="max_length",
#     )

#     # ä¿å­˜æ¯ç­† tokenized çµæœå°æ‡‰åŸå§‹æ¨£æœ¬çš„ç´¢å¼•
#     sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
#     offset_mapping = tokenized_examples.pop("offset_mapping")

#     start_positions = []
#     end_positions = []

#     for i, offsets in enumerate(offset_mapping):
#         sample_index = sample_mapping[i]
#         answers = examples["answers"][sample_index]  # answers ç‚º listï¼ˆå¯èƒ½ç‚ºç©ºï¼‰
#         if len(answers) == 0:
#             start_positions.append(0)
#             end_positions.append(0)
#         else:
#             answer = answers[0]
#             answer_start = answer["answer_start"]
#             answer_text = answer["text"]
#             answer_end = answer_start + len(answer_text)

#             sequence_ids = tokenized_examples.sequence_ids(i)
#             token_start_index = 0
#             while token_start_index < len(offsets) and sequence_ids[token_start_index] != 1:
#                 token_start_index += 1
#             token_end_index = len(offsets) - 1
#             while token_end_index >= 0 and sequence_ids[token_end_index] != 1:
#                 token_end_index -= 1

#             if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):
#                 start_positions.append(0)
#                 end_positions.append(0)
#             else:
#                 while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:
#                     token_start_index += 1
#                 start_positions.append(token_start_index - 1)

#                 while token_end_index >= 0 and offsets[token_end_index][1] >= answer_end:
#                     token_end_index -= 1
#                 end_positions.append(token_end_index + 1)

#     tokenized_examples["start_positions"] = start_positions
#     tokenized_examples["end_positions"] = end_positions
#     return tokenized_examples

# tokenized_train = flat_train_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])
# tokenized_dev = flat_dev_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])

# print("Tokenization èˆ‡æ¨™ç±¤æº–å‚™å¾Œè¨“ç·´è³‡æ–™ç¬¬ä¸€ç­†ï¼š")
# print(tokenized_train[0])

# # -------------------------------
# # 4. Adapter æ¨¡å‹è¼‰å…¥èˆ‡è¨“ç·´è¨­å®š
# # -------------------------------
# adapter_model = AutoAdapterModel.from_pretrained("deepset/roberta-base-squad2")
# adapter_model.add_adapter("qa_adapter", config="pfeiffer")
# adapter_model.train_adapter("qa_adapter")  # è¨­å®šåªè¨“ç·´ adapter éƒ¨åˆ†
# adapter_model.to(device)

# training_args = TrainingArguments(
#     output_dir="./adapter_results",
#     evaluation_strategy="epoch",  # å¦‚ä¸éœ€è¦é »ç¹è©•ä¼°ï¼Œå¯æ”¹ç‚º "no"
#     learning_rate=3e-3,
#     per_device_train_batch_size=16,
#     per_device_eval_batch_size=16,
#     num_train_epochs=10,           # æ¸¬è©¦æ™‚å¯æš«æ™‚é™ä½ epoch æ•¸
#     weight_decay=0.01,
#     save_total_limit=2,
#     save_strategy="epoch",
#     load_best_model_at_end=True,
#     fp16=True if device=="cuda" else False
# )

# trainer = AdapterTrainer(
#     model=adapter_model,
#     args=training_args,
#     train_dataset=tokenized_train,
#     eval_dataset=tokenized_dev
# )

# # é–‹å§‹ Adapter è¨“ç·´
# trainer.train()

# # å„²å­˜è¨“ç·´å¾Œçš„ adapter æ¬Šé‡åˆ° Drive
# adapter_model.save_adapter("/content/drive/MyDrive/adapter-qa", "qa_adapter")

# # -------------------------------
# # 5. å•ç­”æ¨è«– (QA Inference)
# # -------------------------------
# adapter_model.load_adapter("/content/drive/MyDrive/adapter-qa", "qa_adapter")
# adapter_model.set_active_adapters("qa_adapter")
# qa_pipeline = pipeline("question-answering", model=adapter_model, tokenizer=tokenizer)

# with open("/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json", "r", encoding="utf-8") as f:
#     covid_qa_dev = json.load(f)

# predictions = {}
# for entry in covid_qa_dev["data"]:
#     for paragraph in entry["paragraphs"]:
#         context = paragraph["context"]
#         for qa in paragraph["qas"]:
#             question = qa["question"]
#             qid = qa["id"]
#             pred = qa_pipeline(question=question, context=context)
#             predictions[qid] = pred["answer"]

# with open("/content/drive/MyDrive/HW2/pred.json", "w", encoding="utf-8") as f:
#     json.dump(predictions, f, indent=4, ensure_ascii=False)

# print("å‰ 5 ç­†æ¨è«–çµæœï¼š")
# for idx, (qid, answer) in enumerate(predictions.items()):
#     if idx >= 5:
#         break
#     print(f"ID: {qid} -> ç­”æ¡ˆ: {answer}")

# # æ›è¼‰ Google Drive
# from google.colab import drive
# drive.mount('/content/drive')

# # å®‰è£æ‰€éœ€å¥—ä»¶ï¼ˆå¦‚æœå°šæœªå®‰è£ï¼‰
# !pip install transformers datasets adapter-transformers

# import torch
# import json
# from datasets import load_dataset
# from transformers import AutoTokenizer, TrainingArguments
# from adapters import AutoAdapterModel, AdapterTrainer  # Adapter Transformers å¥—ä»¶

# # è¨­å®šè¨­å‚™
# device = "cuda" if torch.cuda.is_available() else "cpu"
# print("ä½¿ç”¨è¨­å‚™ï¼š", device)

# # -------------------------------
# # 1. è¼‰å…¥ COVID-QA è³‡æ–™é›†
# # -------------------------------
# dataset = load_dataset(
#     "json",
#     data_files={
#         "train": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-train.json",
#         "dev": "/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json"
#     }
# )

# print("åŸå§‹è¨“ç·´è³‡æ–™ç¬¬ä¸€ç­†ï¼š")
# print(dataset["train"][0])

# # -------------------------------
# # 2. æ‰å¹³åŒ– JSON çµæ§‹
# # -------------------------------
# def flatten_examples_batched(batch):
#     """å°‡ JSON çµæ§‹è½‰æ›ç‚º `question`ã€`context`ã€`answers` æ ¼å¼"""
#     new_batch = {"question": [], "context": [], "id": [], "answers": []}
#     for data_item in batch["data"]:
#         for item in (data_item if isinstance(data_item, list) else [data_item]):
#             for paragraph in item["paragraphs"]:
#                 context = paragraph["context"]
#                 for qa in paragraph["qas"]:
#                     new_batch["question"].append(qa["question"])
#                     new_batch["context"].append(context)
#                     new_batch["id"].append(qa["id"])
#                     new_batch["answers"].append(qa.get("answers", []))
#     return new_batch

# flat_train_dataset = dataset["train"].map(flatten_examples_batched, batched=True, remove_columns=["data"])
# flat_dev_dataset = dataset["dev"].map(flatten_examples_batched, batched=True, remove_columns=["data"])

# print("æ‰å¹³åŒ–å¾Œè¨“ç·´è³‡æ–™ç¬¬ä¸€ç­†ï¼š")
# print(flat_train_dataset[0])

# # -------------------------------
# # 3. Tokenization èˆ‡æ¨™è¨»ç­”æ¡ˆä½ç½®
# # -------------------------------
# tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2", use_fast=True)

# def prepare_train_features(examples):
#     """Tokenizationï¼Œä¸¦æ¨™è¨»ç­”æ¡ˆèµ·å§‹èˆ‡çµæŸä½ç½®"""
#     tokenized_examples = tokenizer(
#         examples["question"],
#         examples["context"],
#         truncation="only_second",
#         max_length=512,  # ç¢ºä¿ä¸æˆªæ–·ç­”æ¡ˆ
#         stride=128,      # é¿å…éåº¦åˆ‡å‰²
#         return_overflowing_tokens=True,
#         return_offsets_mapping=True,
#         padding="max_length",
#     )

#     sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
#     offset_mapping = tokenized_examples.pop("offset_mapping")

#     start_positions, end_positions = [], []

#     for i, offsets in enumerate(offset_mapping):
#         sample_index = sample_mapping[i]
#         answers = examples["answers"][sample_index]
#         if len(answers) == 0:
#             start_positions.append(0)
#             end_positions.append(0)
#         else:
#             answer = answers[0]
#             answer_start = answer["answer_start"]
#             answer_end = answer_start + len(answer["text"])

#             sequence_ids = tokenized_examples.sequence_ids(i)
#             token_start_index = next(idx for idx, sid in enumerate(sequence_ids) if sid == 1)
#             token_end_index = len(offsets) - 1 - next(idx for idx, sid in enumerate(sequence_ids[::-1]) if sid == 1)

#             if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):
#                 start_positions.append(0)
#                 end_positions.append(0)
#             else:
#                 while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:
#                     token_start_index += 1
#                 start_positions.append(token_start_index - 1)

#                 while token_end_index >= 0 and offsets[token_end_index][1] >= answer_end:
#                     token_end_index -= 1
#                 end_positions.append(token_end_index + 1)

#     tokenized_examples["start_positions"] = start_positions
#     tokenized_examples["end_positions"] = end_positions
#     return tokenized_examples

# tokenized_train = flat_train_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])
# tokenized_dev = flat_dev_dataset.map(prepare_train_features, batched=True, remove_columns=["question", "context", "id", "answers"])

# print("Tokenization å¾Œçš„è¨“ç·´è³‡æ–™ç¬¬ä¸€ç­†ï¼š")
# print(tokenized_train[0])

# # -------------------------------
# # 4. Adapter æ¨¡å‹è¼‰å…¥èˆ‡è¨“ç·´
# # -------------------------------
# adapter_model = AutoAdapterModel.from_pretrained("deepset/roberta-base-squad2")
# adapter_model.add_adapter("qa_adapter", config="pfeiffer")
# adapter_model.train_adapter("qa_adapter")
# adapter_model.to(device)

# training_args = TrainingArguments(
#     output_dir="./adapter_results",
#     evaluation_strategy="epoch",
#     learning_rate=3e-3,
#     per_device_train_batch_size=16,
#     per_device_eval_batch_size=16,
#     num_train_epochs=10,
#     weight_decay=0.01,
#     save_total_limit=2,
#     save_strategy="epoch",
#     load_best_model_at_end=True,
#     fp16=True if device == "cuda" else False
# )

# trainer = AdapterTrainer(
#     model=adapter_model,
#     args=training_args,
#     train_dataset=tokenized_train,
#     eval_dataset=tokenized_dev
# )

# trainer.train()

# # å„²å­˜è¨“ç·´å¾Œçš„ Adapter æ¬Šé‡
# adapter_model.save_adapter("/content/drive/MyDrive/adapter-qa", "qa_adapter")

# # -------------------------------
# # 5. å•ç­”æ¨è«– (QA Inference)
# # -------------------------------
# adapter_model.load_adapter("/content/drive/MyDrive/adapter-qa", "qa_adapter")
# adapter_model.set_active_adapters(["qa_adapter"])
# adapter_model.to(device)

# def answer_question(model, tokenizer, question, context):
#     """æ‰‹å‹•æ¨è«–å‡½æ•¸"""
#     inputs = tokenizer(question, context, return_tensors="pt", truncation=True).to(device)
#     with torch.no_grad():
#         outputs = model(**inputs)
#     answer_start = torch.argmax(outputs.start_logits)
#     answer_end = torch.argmax(outputs.end_logits) + 1
#     answer = tokenizer.convert_tokens_to_string(
#         tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end])
#     )
#     return answer

# with open("/content/drive/MyDrive/HW2/covid-qa/covid-qa-dev.json", "r", encoding="utf-8") as f:
#     covid_qa_dev = json.load(f)

# predictions = {}
# for entry in covid_qa_dev["data"]:
#     for paragraph in entry["paragraphs"]:
#         context = paragraph["context"]
#         for qa in paragraph["qas"]:
#             question = qa["question"]
#             qid = qa["id"]
#             pred_answer = answer_question(adapter_model, tokenizer, question, context)
#             predictions[qid] = pred_answer

# # å„²å­˜é æ¸¬çµæœ
# with open("/content/drive/MyDrive/HW2/pred.json", "w", encoding="utf-8") as f:
#     json.dump(predictions, f, indent=4, ensure_ascii=False)

# # é¡¯ç¤ºå‰ 5 ç­†çµæœ
# print("å‰ 5 ç­†æ¨è«–çµæœï¼š")
# for idx, (qid, answer) in enumerate(predictions.items()):
#     if idx >= 5:
#         break
#     print(f"ID: {qid} -> ç­”æ¡ˆ: {answer}")